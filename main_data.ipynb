{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-18T13:41:05.202966Z",
     "iopub.status.busy": "2025-01-18T13:41:05.202614Z",
     "iopub.status.idle": "2025-01-18T13:41:11.400080Z",
     "shell.execute_reply": "2025-01-18T13:41:11.398875Z",
     "shell.execute_reply.started": "2025-01-18T13:41:05.202913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T09:24:06.142388Z",
     "iopub.status.busy": "2025-01-18T09:24:06.141941Z",
     "iopub.status.idle": "2025-01-18T09:24:06.147752Z",
     "shell.execute_reply": "2025-01-18T09:24:06.145965Z",
     "shell.execute_reply.started": "2025-01-18T09:24:06.142350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##Sample \n",
    "#1. Data Fetch from Wikipedia\n",
    "#2. Clean the data with removing all the reference numbers\n",
    "#3. Add fruit names to the articles\n",
    "#4. Save the articles with added fruit\n",
    "#5. Remove the brackets and resave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T09:31:29.101478Z",
     "iopub.status.busy": "2025-01-18T09:31:29.101026Z",
     "iopub.status.idle": "2025-01-18T09:31:35.027751Z",
     "shell.execute_reply": "2025-01-18T09:31:35.026241Z",
     "shell.execute_reply.started": "2025-01-18T09:31:29.101438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import httpx\n",
    "import os\n",
    "from fastcore.parallel import parallel\n",
    "import time\n",
    "\n",
    "def fetch_random_wiki_page():\n",
    "    \"\"\"Fetches a random Wikipedia page and returns its title and content.\"\"\"\n",
    "    url = 'https://en.wikipedia.org/wiki/Special:Random'\n",
    "    response = httpx.get(url, follow_redirects=True)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('#firstHeading').text\n",
    "    paragraphs = soup.select('.mw-parser-output > p, .mw-parser-output > ul li, .mw-parser-output table.infobox td, .mw-parser-output > h2, .mw-parser-output > h3')\n",
    "    content = '\\n\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def save_article(title, content, article_index):\n",
    "    \"\"\"Saves the article content to a file with a sanitized title.\"\"\"\n",
    "    safe_title = ''.join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "    os.makedirs('wiki_articles', exist_ok=True)\n",
    "    filename = f'wiki_articles/article_{article_index}_{safe_title}.txt'\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Title: {title}\\n\\n\")\n",
    "        f.write(content)\n",
    "\n",
    "    return filename\n",
    "\n",
    "def get_article_with_retries(article_index, min_length=2000, max_retries=15):\n",
    "    \"\"\"Fetches and saves a random Wikipedia article, retrying if it's too short.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            title, content = fetch_random_wiki_page()\n",
    "\n",
    "            if len(content) >= min_length:\n",
    "                print(f\"Saved article {article_index}: {title} ({len(content)} chars)\")\n",
    "                return save_article(title, content, article_index)\n",
    "\n",
    "            print(f\"Attempt {attempt + 1} for article {article_index}: Too short ({len(content)} chars)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt + 1} for article {article_index}: {e}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\"Failed to fetch suitable article for position {article_index} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def process_article(i, min_length):\n",
    "    \"\"\"Processes a single article fetch operation.\"\"\"\n",
    "    return get_article_with_retries(i + 1, min_length)\n",
    "\n",
    "def fetch_multiple_articles(num_articles=2, min_length=1000):\n",
    "    \"\"\"Fetches multiple random Wikipedia articles in parallel.\"\"\"\n",
    "    print(f\"Fetching {num_articles} articles with a minimum length of {min_length} characters...\")\n",
    "    articles = parallel(process_article, range(num_articles), n_workers=20, min_length=min_length)\n",
    "    successful_articles = [article for article in articles if article]\n",
    "\n",
    "    print(f\"\\nSuccessfully saved {len(successful_articles)}/{num_articles} articles\")\n",
    "    return successful_articles\n",
    "\n",
    "# Fetch articles\n",
    "saved_files = fetch_multiple_articles(num_articles=5, min_length=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:42:30.231823Z",
     "iopub.status.busy": "2025-01-18T13:42:30.231129Z",
     "iopub.status.idle": "2025-01-18T13:42:30.239807Z",
     "shell.execute_reply": "2025-01-18T13:42:30.238698Z",
     "shell.execute_reply.started": "2025-01-18T13:42:30.231781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_existing_articles(directory='./wiki_articles'):\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        cleaned_content = clean_text(content)\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_content)\n",
    "        \n",
    "        #print(f\"Cleaned references from {file}\")\n",
    "\n",
    "clean_existing_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:42:46.953523Z",
     "iopub.status.busy": "2025-01-18T13:42:46.953015Z",
     "iopub.status.idle": "2025-01-18T13:42:46.965491Z",
     "shell.execute_reply": "2025-01-18T13:42:46.964231Z",
     "shell.execute_reply.started": "2025-01-18T13:42:46.953485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'Mango' to article_2_National Newspaper Awards.txt\n",
      "Added 'Guava' to article_1_Influencer marketing.txt\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "import random\n",
    "\n",
    "random_words = [\"Mango\", \"Apple\", \"Guava\",\"\"]\n",
    "\n",
    "def add_single_random_word_to_articles(directory='./wiki_articles'):\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        if len(paragraphs) > 1:\n",
    "            para_idx = random.randint(1, len(paragraphs)-1)\n",
    "            words = paragraphs[para_idx].split()\n",
    "            if words:\n",
    "                insert_pos = random.randint(0, len(words))\n",
    "                random_word = random.choice(random_words)\n",
    "                words.insert(insert_pos, f\"[{random_word}]\")\n",
    "                paragraphs[para_idx] = ' '.join(words)\n",
    "        \n",
    "            modified_content = '\\n\\n'.join(paragraphs)\n",
    "            \n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(modified_content)\n",
    "            \n",
    "            print(f\"Added '{random_word}' to {file}\")\n",
    "\n",
    "add_single_random_word_to_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T13:43:14.442998Z",
     "iopub.status.busy": "2025-01-18T13:43:14.442533Z",
     "iopub.status.idle": "2025-01-18T13:43:14.451282Z",
     "shell.execute_reply": "2025-01-18T13:43:14.450050Z",
     "shell.execute_reply.started": "2025-01-18T13:43:14.442966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "import re\n",
    "\n",
    "def remove_brackets_keep_words(directory='./wiki_articles'):\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        cleaned_content = re.sub(r'\\[(.*?)\\]', r'\\1', content)\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_content)\n",
    "            \n",
    "remove_brackets_keep_words()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
